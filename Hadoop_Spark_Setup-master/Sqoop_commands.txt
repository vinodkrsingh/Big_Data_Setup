ls /opt/cloudera/parcels/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/sqoop/bin

sqoop help

sqoop list-databases --connect jdbc:mysql://10.0.1.10 --username labuser -P
sqoop import --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P --table help_keyword

sqoop import --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P --table help_keyword --m 1 --target-
dir /user/vinodkumar/helpk1

sqoop import --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P --table help_keyword --m 6 --target-
dir /user/vinodkumar/helpk2

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --m 6 --target-dir /user/vinodkumar/helpk3

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --m 1 --target-dir /user/vinodkumar/helpk2 -z

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --m 1 --target-dir /user/vinodkumar/helpk3 --compression-codec 
org.apache.hadoop.io.compress.SnappyCodec

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --m 1 --target-dir /user/vinodkumar/helpk4 --compression-codec 
org.apache.hadoop.io.compress.BZip2Codec

cat /etc/hadoop/conf/core-site.xml | grep compress

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --m 1 --target-dir /user/vinodkumar/helpk5 --compression-codec org.apache.hadoop.io.compress.Lz4Codec

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --m 1 --target-dir /user/vinodkumar/helpk6 --compression-codec org.apache.hadoop.io.compress.DefaultCodec 

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql 
--username labuser -P -
-m 1 --target-dir /user/vinodkumar/helpk9 --query 
"select help_keyword_id from help_keyword 
where help_keyword_id > 300 and \$CONDITIONS"

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-m 1 --table help_keyword --target-dir /user/vinodkumar/helpk10 --as-sequencefile

sqoop import "-Dmapreduce.framework.name=local" "-Dmapreduce.output.fileoutputformat.compress=true" "-Dmapred.
output.compression.type=RECORD" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P --m 1 --table 
help_keyword --target-dir /user/vinodkumar/helpk12 --as-sequencefile

sqoop import "-Dmapreduce.framework.name=local" "-Dmapreduce.output.fileoutputformat.compress=true" "-Dmapred.
output.compression.type=BLOCK" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P --m 1 --table help_keyword --target-dir /user/vinodkumar/helpk11 --as-sequencefile

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-m 1 --table help_keyword --target-dir /user/vinodkumar/helpk13 --as-avrodatafile

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-m 1 --table help_keyword --target-dir /user/vinodkumar/helpk14 --as-parquetfile ****** to check

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/testaj --username labuser -P 
--table testajtbl2 --target-dir /user/vinodkumar/helpk15 --split-by help_keyword_id

sqoop import "-Dmapreduce.framework.name=local" "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" --conne
ct jdbc:mysql://10.0.1.10/testaj --username labuser -P --table testajtbl2 --target-dir /user/vinodkumar/helpk17 
--split-by name
----------------------------
importing into hbase and testing..

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --target-dir /user/vinodkumar/helpk19 --hbase-create-table --hbase-table helpnewhbtbl --column-family impinfo

hbase shell
scan 'helpnewhbtbl'
get 'helpnewhbtbl', '98'

-----------------------
importing into hive and testing...
sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/mysql --username labuser -P -
-table help_keyword --target-dir /user/vinodkumar/helpk18 --hive-import --create-hive-table --hive-table helpnewhvtbl


hive
select * from helpnewhvtbl limit 10;
describe extended helpnewhvtbl; 
select * from helpnewhvtbl where help_keyword_id > 600 order by help_keyword_id desc;

export into mysql
sqoop export "-Dmapreduce.framework.name=local" --connect jdbc:mysql://10.0.1.10/testaj 
--username labuser -P 
--table testajtbl3 --export-dir /user/vinodkumar/helpk1
============================
